{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c154a88",
   "metadata": {},
   "source": [
    "# 1.Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24004826",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc31caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd95b91",
   "metadata": {},
   "source": [
    "## 1.2 Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e96b1",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load our train data into a dataFrame\n",
    "trainDf = pd.read_excel('resources/referentiel_foodex.xlsx',sheet_name='Feuil1')\n",
    "trainDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811c8d4",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2960dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test.json into an Excel file to be easy to annoutate.\n",
    "with open(\"resources/test.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "products = data[\"designations\"]\n",
    "testDf = pd.DataFrame({\n",
    "    \"Product\": products,\n",
    "    \"Category_clean\": [\"\"] * len(products)\n",
    "})\n",
    "testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56911e94",
   "metadata": {},
   "source": [
    "## 1.3 Cleaning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb97173",
   "metadata": {},
   "source": [
    "### Duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cccf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are duplicated rows \n",
    "# No duplicated rows in train\n",
    "\n",
    "duplicates = trainDf[trainDf.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No duplicated rows in test\n",
    "\n",
    "duplicates = testDf[testDf.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3565fb",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No missing values in test\n",
    "testDf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 missing values in train\n",
    "trainDf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There 3 missing categories, for the moment we just remove them \n",
    "#-> (any row that has a missing value in a column will be dropped)\n",
    "\n",
    "trainDf_Cleaned = trainDf.dropna()\n",
    "trainDf_Cleaned = trainDf_Cleaned.rename(columns={\n",
    "    'Désignation commerciale':'Product',\n",
    "    'Catégorie de référence':'Category'})\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8613c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned = testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec611cb6",
   "metadata": {},
   "source": [
    "### Scientific names handeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_scientific_names(text):\n",
    "    # Use regex to find and remove all text between parentheses (scientific names)\n",
    "    cleaned_text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a023f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned[\"Product_clean\"] = trainDf_Cleaned[\"Product\"].apply(remove_scientific_names)\n",
    "trainDf_Cleaned[\"Category_clean\"] = trainDf_Cleaned[\"Category\"].apply(remove_scientific_names)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce104a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product\"].apply(remove_scientific_names)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a6035",
   "metadata": {},
   "source": [
    "### Special caracters and Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9àâäéèêëîïôöùûüÿçœæÀÂÄÉÈÊËÎÏÔÖÙÛÜŸÇŒÆ\\s]\",\" \",text) # Replace special caracters with white space.\n",
    "    return text.lower() # lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned[\"Product_clean\"] = trainDf_Cleaned[\"Product_clean\"].apply(cleanText)\n",
    "trainDf_Cleaned[\"Category_clean\"] = trainDf_Cleaned[\"Category_clean\"].apply(cleanText)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d2849",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product_clean\"].apply(cleanText)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17372f9e",
   "metadata": {},
   "source": [
    "### Stopwords removing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download French stopwords if not already\n",
    "nltk.download('stopwords')\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "len(french_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79aee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_french_stopwords(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return ' '.join([w for w in words if w not in french_stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to both columns\n",
    "trainDf_Cleaned['Product_clean'] = trainDf_Cleaned['Product_clean'].apply(remove_french_stopwords)\n",
    "trainDf_Cleaned['Category_clean'] = trainDf_Cleaned['Category_clean'].apply(remove_french_stopwords)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product_clean\"].apply(remove_french_stopwords)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456994ac",
   "metadata": {},
   "source": [
    "### Keep only Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186ae8c",
   "metadata": {},
   "source": [
    "use the following command to install the model :\n",
    "\n",
    "```bash\n",
    "uv run python -m spacy download fr_dep_news_trf \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    cleanedText = \" \".join([token.text for token in doc if token.pos_ in [\"NOUN\",\"PROPN\"] ])\n",
    "    if len(cleanedText) > 0:\n",
    "        return cleanedText\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47991099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "\n",
    "print(keep_nouns(\"boissons au cola caféiniques faibles en \"))\n",
    "\n",
    "doc = nlp(\"boissons au cola caféiniques faibles en \")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2638458",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned[\"Product_clean\"] = trainDf_Cleaned[\"Product_clean\"].apply(keep_nouns)\n",
    "trainDf_Cleaned[\"Category_clean\"] = trainDf_Cleaned[\"Category_clean\"].apply(keep_nouns)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product_clean\"].apply(keep_nouns)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0581fb0",
   "metadata": {},
   "source": [
    "### Redundant words handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_words(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    cleaned_text = ' '.join(sorted(unique_words, key=words.index))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned[\"Product_clean\"] = trainDf_Cleaned[\"Product_clean\"].apply(remove_redundant_words)\n",
    "trainDf_Cleaned[\"Category_clean\"] = trainDf_Cleaned[\"Category_clean\"].apply(remove_redundant_words)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product_clean\"].apply(remove_redundant_words)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a4104",
   "metadata": {},
   "source": [
    "### Save the cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned.iloc[:, -2:].to_excel('data/train_cleaned.xlsx',index=False)\n",
    "testDf_Cleaned.to_excel('data/test_cleaned.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb43da1",
   "metadata": {},
   "source": [
    "# 2.Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88a981",
   "metadata": {},
   "source": [
    "## 2.1 Evaluation function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824528cf",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0529b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned = pd.read_excel('data/test_cleaned.xlsx')\n",
    "trainDf_Cleaned = pd.read_excel('data/train_cleaned.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae04d23",
   "metadata": {},
   "source": [
    "### Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map knowen categories from train data by merging on Product_clean column\n",
    "\n",
    "testDf_Cleaned = testDf_Cleaned.drop(columns=[\"Category_clean\"])  # drop the empty column\n",
    "testDf_Cleaned = testDf_Cleaned.merge(trainDf_Cleaned, on=\"Product_clean\", how=\"left\")\n",
    "testDf_Cleaned.to_excel('data/test_cleaned.xlsx',index=False)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data after annoutating remained Products\n",
    "testDf_Cleaned = pd.read_excel('data/test_cleaned.xlsx')\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20068e5",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f936f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(true_df, pred_df):\n",
    "    merged = true_df.merge(pred_df, on=\"Product_clean\", how=\"inner\")\n",
    "    y_true = merged[\"Category_clean\"]\n",
    "    y_pred = merged[\"Category_predicted\"]\n",
    "    return accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e11300",
   "metadata": {},
   "source": [
    "## 2.2 Keywords based Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c046c1c",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = pd.read_excel('data/train_cleaned.xlsx')\n",
    "trainDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde597d5",
   "metadata": {},
   "source": [
    "### Get Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Candidates(productName):\n",
    "    categories = trainDf['Category_clean'].tolist()\n",
    "    candidates = set()\n",
    "    keywords = productName.strip().split()\n",
    "    for keyword in keywords:\n",
    "        for category in categories:\n",
    "            if keyword in category.strip().split():\n",
    "                candidates.add(category)\n",
    "    return list(candidates)\n",
    "\n",
    "def runAll():\n",
    "    products = trainDf['Product_clean'].tolist()\n",
    "    candidates_list = []\n",
    "    for product in products:\n",
    "        candidates = get_Candidates(product)\n",
    "        candidates_list.append(candidates)\n",
    "    trainDf['Candidate_categories'] = candidates_list\n",
    "    return trainDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "get_Candidates(\"purée pommes terre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c83e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates = runAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d290f",
   "metadata": {},
   "source": [
    "### Set predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c48095",
   "metadata": {},
   "source": [
    "If a product has only one candidate, there is no need for the refinement step, as the predicted category is already determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates['Category_predicted'] = train_with_candidates['Candidate_categories'].apply(\n",
    "    lambda x: x[0] if len(x) == 1 else ''\n",
    ")\n",
    "train_with_candidates.to_excel('data/train_with_candidates.xlsx',index=False)\n",
    "train_with_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca028fb4",
   "metadata": {},
   "source": [
    "## 2.3 Embeddings model approach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b36b6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cb90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4f7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d636f063c7048c9930f3ae123434dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\GitRepos\\test_technique_chemosens\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--Lajavaness--bilingual-embedding-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df83ae4fa734a5eaa7c9dffbe69dba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/176 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ca8793af044393845ec2917f606904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc5c31c1d0e4e0592177bc83a85cc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898d2ec657df41cf81950c381e4bd420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a10d284952b4a42bd7a5a0d42312e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\GitRepos\\test_technique_chemosens\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--dangvantuan--bilingual_impl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "A new version of the following files was downloaded from https://huggingface.co/dangvantuan/bilingual_impl:\n",
      "- config.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9600034e5eaf4a05bd6acf75f882b26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/dangvantuan/bilingual_impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4a6b5b3d954c5aa1c2574dea6fbdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('Lajavaness/bilingual-embedding-large', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc7e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "251159ee",
   "metadata": {},
   "source": [
    "## 2.4 TF-IDF + SVM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2896c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375e355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-technique-chemosens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
