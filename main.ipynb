{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c154a88",
   "metadata": {},
   "source": [
    "# 1.Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24004826",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc31caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd95b91",
   "metadata": {},
   "source": [
    "## 1.2 Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e96b1",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load our train data into a dataFrame\n",
    "trainDf = pd.read_excel('resources/referentiel_foodex.xlsx',sheet_name='Feuil1')\n",
    "trainDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811c8d4",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2960dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test.json into an Excel file to be easy to annoutate.\n",
    "with open(\"resources/test.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "products = data[\"designations\"]\n",
    "testDf = pd.DataFrame({\n",
    "    \"Product\": products,\n",
    "    \"Category_clean\": [\"\"] * len(products)\n",
    "})\n",
    "testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56911e94",
   "metadata": {},
   "source": [
    "## 1.3 Cleaning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb97173",
   "metadata": {},
   "source": [
    "### Duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cccf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are duplicated rows \n",
    "# No duplicated rows in train\n",
    "\n",
    "duplicates = trainDf[trainDf.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No duplicated rows in test\n",
    "\n",
    "duplicates = testDf[testDf.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3565fb",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No missing values in test\n",
    "testDf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 missing values in train\n",
    "trainDf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There 3 missing categories, for the moment we just remove them \n",
    "#-> (any row that has a missing value in a column will be dropped)\n",
    "\n",
    "trainDf_Cleaned = trainDf.dropna()\n",
    "trainDf_Cleaned = trainDf_Cleaned.rename(columns={\n",
    "    'Désignation commerciale':'Product',\n",
    "    'Catégorie de référence':'Category'})\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8613c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned = testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec611cb6",
   "metadata": {},
   "source": [
    "### Scientific names handeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_scientific_names(text):\n",
    "    # Use regex to find and remove all text between parentheses (scientific names)\n",
    "    cleaned_text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a023f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned[\"Product_clean\"] = trainDf_Cleaned[\"Product\"].apply(remove_scientific_names)\n",
    "trainDf_Cleaned[\"Category_clean\"] = trainDf_Cleaned[\"Category\"].apply(remove_scientific_names)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce104a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product\"].apply(remove_scientific_names)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a6035",
   "metadata": {},
   "source": [
    "### Special caracters and Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9àâäéèêëîïôöùûüÿçœæÀÂÄÉÈÊËÎÏÔÖÙÛÜŸÇŒÆ\\s]\",\" \",text) # Replace special caracters with white space.\n",
    "    return text.lower() # lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned[\"Product_clean\"] = trainDf_Cleaned[\"Product_clean\"].apply(cleanText)\n",
    "trainDf_Cleaned[\"Category_clean\"] = trainDf_Cleaned[\"Category_clean\"].apply(cleanText)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d2849",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product_clean\"].apply(cleanText)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17372f9e",
   "metadata": {},
   "source": [
    "### Stopwords removing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download French stopwords if not already\n",
    "nltk.download('stopwords')\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "len(french_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79aee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_french_stopwords(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return ' '.join([w for w in words if w not in french_stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to both columns\n",
    "trainDf_Cleaned['Product_clean'] = trainDf_Cleaned['Product_clean'].apply(remove_french_stopwords)\n",
    "trainDf_Cleaned['Category_clean'] = trainDf_Cleaned['Category_clean'].apply(remove_french_stopwords)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product_clean\"].apply(remove_french_stopwords)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456994ac",
   "metadata": {},
   "source": [
    "### Keep only Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186ae8c",
   "metadata": {},
   "source": [
    "use the following command to install the model :\n",
    "\n",
    "```bash\n",
    "uv run python -m spacy download fr_dep_news_trf \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    cleanedText = \" \".join([token.text for token in doc if token.pos_ in [\"NOUN\",\"PROPN\"] ])\n",
    "    if len(cleanedText) > 0:\n",
    "        return cleanedText\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47991099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "\n",
    "print(keep_nouns(\"boissons au cola caféiniques faibles en \"))\n",
    "\n",
    "doc = nlp(\"boissons au cola caféiniques faibles en \")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2638458",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned[\"Product_clean\"] = trainDf_Cleaned[\"Product_clean\"].apply(keep_nouns)\n",
    "trainDf_Cleaned[\"Category_clean\"] = trainDf_Cleaned[\"Category_clean\"].apply(keep_nouns)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product_clean\"].apply(keep_nouns)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0581fb0",
   "metadata": {},
   "source": [
    "### Redundant words handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_words(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    cleaned_text = ' '.join(sorted(unique_words, key=words.index))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned[\"Product_clean\"] = trainDf_Cleaned[\"Product_clean\"].apply(remove_redundant_words)\n",
    "trainDf_Cleaned[\"Category_clean\"] = trainDf_Cleaned[\"Category_clean\"].apply(remove_redundant_words)\n",
    "trainDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned[\"Product_clean\"] = testDf_Cleaned[\"Product_clean\"].apply(remove_redundant_words)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a4104",
   "metadata": {},
   "source": [
    "### Save the cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf_Cleaned.iloc[:, -2:].to_excel('data/train_cleaned.xlsx',index=False)\n",
    "testDf_Cleaned.to_excel('data/test_cleaned.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb43da1",
   "metadata": {},
   "source": [
    "# 2.Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88a981",
   "metadata": {},
   "source": [
    "## 2.1 Evaluation function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824528cf",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0529b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf_Cleaned = pd.read_excel('data/test_cleaned.xlsx')\n",
    "trainDf_Cleaned = pd.read_excel('data/train_cleaned.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae04d23",
   "metadata": {},
   "source": [
    "### Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map knowen categories from train data by merging on Product_clean column\n",
    "\n",
    "testDf_Cleaned = testDf_Cleaned.drop(columns=[\"Category_clean\"])  # drop the empty column\n",
    "testDf_Cleaned = testDf_Cleaned.merge(trainDf_Cleaned, on=\"Product_clean\", how=\"left\")\n",
    "testDf_Cleaned.to_excel('data/test_cleaned.xlsx',index=False)\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data after annoutating remained Products\n",
    "testDf_Cleaned = pd.read_excel('data/test_cleaned.xlsx')\n",
    "testDf_Cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20068e5",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f936f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(true_df,pred_df):\n",
    "    merged = true_df.merge(pred_df, on=\"Product_clean\", how=\"inner\")\n",
    "    y_true = merged[\"Category_clean\"]\n",
    "    y_pred = merged[\"Category_predicted\"]\n",
    "    return accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e11300",
   "metadata": {},
   "source": [
    "## 2.2 Keywords based Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c046c1c",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned = pd.read_excel('data/train_cleaned.xlsx')\n",
    "test_cleaned = pd.read_excel('data/test_cleaned.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde597d5",
   "metadata": {},
   "source": [
    "### Get Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Candidates(productName,dataframe):\n",
    "    categories = dataframe['Category_clean'].unique().tolist()\n",
    "    candidates = set()\n",
    "    keywords = productName.strip().split()\n",
    "    for keyword in keywords:\n",
    "        for category in categories:\n",
    "            if keyword in category.strip().split():\n",
    "                candidates.add(category)\n",
    "    return list(candidates)\n",
    "\n",
    "def runAll(dataframe):\n",
    "    products = dataframe['Product_clean'].tolist()\n",
    "    candidates_list = []\n",
    "    for product in products:\n",
    "        candidates = get_Candidates(product,dataframe)\n",
    "        candidates_list.append(candidates)\n",
    "    dataframe['Candidate_categories'] = candidates_list\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "get_Candidates(\"purée pommes terre\",train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c83e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates = runAll(train_cleaned)\n",
    "test_with_candidates = runAll(test_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d290f",
   "metadata": {},
   "source": [
    "### Set predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c48095",
   "metadata": {},
   "source": [
    "If a product has only one candidate, there is no need for the refinement step, as the predicted category is already determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates['Category_predicted'] = train_with_candidates['Candidate_categories'].apply(\n",
    "    lambda x: x[0] if len(x) == 1 else ''\n",
    ")\n",
    "train_with_candidates.to_excel('data/train_with_candidates.xlsx',index=False)\n",
    "train_with_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_candidates['Category_predicted'] = test_with_candidates['Candidate_categories'].apply(\n",
    "    lambda x: x[0] if len(x) == 1 else ''\n",
    ")\n",
    "test_with_candidates.to_excel('data/test_with_candidates.xlsx',index=False)\n",
    "test_with_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca028fb4",
   "metadata": {},
   "source": [
    "## 2.3 Embeddings model approach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b36b6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer,util\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd227b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('Lajavaness/bilingual-embedding-large', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStringTolist(dataframe):\n",
    "    ### Convert the string representation of list into a real Python list\n",
    "    \n",
    "    dataframe[\"Candidate_categories\"] = dataframe[\"Candidate_categories\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e2d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates = pd.read_excel(\"data/train_with_candidates.xlsx\")\n",
    "test_with_candidates = pd.read_excel(\"data/test_with_candidates.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf06324",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates = getStringTolist(train_with_candidates)\n",
    "test_with_candidates = getStringTolist(test_with_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9beb8fa",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45664a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_embeddings_approach(dataframe):\n",
    "    \n",
    "    # Encode all unique categories\n",
    "    all_categories = dataframe[\"Category_clean\"].unique().tolist()\n",
    "    all_cat_emb = embedding_model.encode(all_categories, convert_to_tensor=True)\n",
    "    \n",
    "    # Prediction loop\n",
    "    preds = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        if pd.notna(row[\"Category_predicted\"]):\n",
    "            preds.append(row[\"Category_predicted\"])\n",
    "            continue\n",
    "    \n",
    "        candidates = row[\"Candidate_categories\"]\n",
    "        if len(candidates) == 0:\n",
    "            candidates = all_categories\n",
    "            cand_emb = all_cat_emb\n",
    "        else:\n",
    "            cand_emb = embedding_model.encode(candidates, convert_to_tensor=True)\n",
    "\n",
    "        prod_emb = embedding_model.encode(row[\"Product_clean\"], convert_to_tensor=True)\n",
    "        sims = util.cos_sim(prod_emb, cand_emb)[0]\n",
    "        best_idx = sims.argmax().item()\n",
    "        preds.append(candidates[best_idx])\n",
    "\n",
    "    dataframe[\"Category_predicted\"] = preds\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates = predict_embeddings_approach(train_with_candidates)\n",
    "test_with_candidates = predict_embeddings_approach(test_with_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd94f3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bdcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On train data\n",
    "\n",
    "trainDf_Cleaned = pd.read_excel('data/train_cleaned.xlsx') \n",
    "train_accuarcy = Evaluate(trainDf_Cleaned,train_with_candidates[[\"Product_clean\", \"Category_predicted\"]])\n",
    "print(\"Accuarcy on train data\",round(train_accuarcy,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc50eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On test data\n",
    "\n",
    "testDf_Cleaned = pd.read_excel('data/test_cleaned.xlsx') \n",
    "test_accuarcy = Evaluate(testDf_Cleaned,test_with_candidates[[\"Product_clean\", \"Category_predicted\"]])\n",
    "print(\"Accuarcy on test data\",round(test_accuarcy,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571c638",
   "metadata": {},
   "source": [
    "### Exporte test rsults "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0af693",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_candidates.to_excel('data/embeddings_approach_test_results.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251159ee",
   "metadata": {},
   "source": [
    "## 2.4 TF-IDF + SVM approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4837bed",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2896c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates = pd.read_excel(\"data/train_with_candidates.xlsx\")\n",
    "test_with_candidates = pd.read_excel(\"data/test_with_candidates.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates = getStringTolist(train_with_candidates)\n",
    "test_with_candidates = getStringTolist(test_with_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbab6b4",
   "metadata": {},
   "source": [
    "### Train the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(train_with_candidates['Product_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM \n",
    "svm = SVC(kernel='linear', probability=True)\n",
    "svm.fit(X_tfidf, train_with_candidates['Category_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9b9ce",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with candidate filtering\n",
    "def predict_tfidf_svm(model, X_vec, candidate_lists):\n",
    "    predictions = []\n",
    "    for i, x in enumerate(X_vec):\n",
    "        probs = model.predict_proba(x)\n",
    "        classes = model.classes_\n",
    "        candidate_classes = candidate_lists[i]\n",
    "        if candidate_classes:  # filter by candidates\n",
    "            mask = [cls in candidate_classes for cls in classes]\n",
    "            filtered_probs = probs[0][mask]\n",
    "            filtered_classes = [cls for cls in classes if cls in candidate_classes]\n",
    "            pred = filtered_classes[filtered_probs.argmax()]\n",
    "        else:  # fallback to all classes\n",
    "            pred = classes[probs[0].argmax()]\n",
    "        predictions.append(pred)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on train\n",
    "X_vecs = [vectorizer.transform([x]) for x in train_with_candidates['Product_clean']]\n",
    "y_pred_train = predict_tfidf_svm(svm, X_vecs, train_with_candidates['Candidate_categories'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad289db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test\n",
    "X_vecs = [vectorizer.transform([x]) for x in test_with_candidates['Product_clean']]\n",
    "y_pred_test = predict_tfidf_svm(svm, X_vecs, test_with_candidates['Candidate_categories'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates['Category_predicted'] = y_pred_train\n",
    "test_with_candidates['Category_predicted'] = y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a05bb",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On train data\n",
    "\n",
    "trainDf_Cleaned = pd.read_excel('data/train_cleaned.xlsx') \n",
    "train_accuarcy = Evaluate(trainDf_Cleaned,train_with_candidates[[\"Product_clean\", \"Category_predicted\"]])\n",
    "print(\"Accuarcy on train data\",round(train_accuarcy,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On test data\n",
    "\n",
    "testDf_Cleaned = pd.read_excel('data/test_cleaned.xlsx') \n",
    "test_accuarcy = Evaluate(testDf_Cleaned,test_with_candidates[[\"Product_clean\", \"Category_predicted\"]])\n",
    "print(\"Accuarcy on test data\",round(test_accuarcy,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5798d2",
   "metadata": {},
   "source": [
    "### Exporte test rsults "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_candidates.to_excel('data/tfidf_svm_approach_test_results.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9554c",
   "metadata": {},
   "source": [
    "## 2.3 LLM approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ae08e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12629472",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates = pd.read_excel(\"data/train_with_candidates.xlsx\")\n",
    "test_with_candidates = pd.read_excel(\"data/test_with_candidates.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27071128",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates = getStringTolist(train_with_candidates)\n",
    "test_with_candidates = getStringTolist(test_with_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbfdb59",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_flash = GoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", \n",
    "    google_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysprompt = \"\"\"\n",
    "You are a product classification assistant.\n",
    "Your task is to output the most appropriate **category** for a given product name.\n",
    "\n",
    "* A list of candidate categories will always be provided. You must choose **only one** category from this list.\n",
    "* Your response must be **only the category name**, in **lowercase**, with no explanation, formatting, or extra text.\n",
    "\"\"\"\n",
    "\n",
    "userprompt = \"\"\"\n",
    "Product name\n",
    "---\n",
    "{product}\n",
    "\n",
    "Candidate categories\n",
    "---\n",
    "{candidates}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", sysprompt),\n",
    "    (\"user\", userprompt)\n",
    "])\n",
    "\n",
    "chain = prompt | gemini_flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "input = {\n",
    "         \"product\":\"lait chocolat\",\n",
    "         \"candidates\":\"['produits chocolat', 'barre chocolat', 'glace base lait', 'chocolat', 'yaourt lait fruits']\",\n",
    "        }\n",
    "print(\"Category is : \",chain.invoke(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99757ec",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_llm(dataframe):\n",
    "    predictions = []\n",
    "    temp = 0\n",
    "    all_categories = dataframe['Category_clean'].unique().tolist()\n",
    "    for product in tqdm(dataframe['Product_clean'].tolist()):\n",
    "        row = dataframe[dataframe['Product_clean'] == product]\n",
    "        candidates = row.iloc[0]['Candidate_categories']\n",
    "        if len(candidates) == 0:\n",
    "            candidates = all_categories\n",
    "        input = {\"product\":product,\"candidates\":candidates}\n",
    "        \n",
    "        temp+=1\n",
    "        if temp >= 13: # API Rate limit of 15 requests per minute, so we pause execution for 1 min then we continue.\n",
    "            print(\"Sleep python for 60 seconds\")\n",
    "            time.sleep(60)\n",
    "            temp=0\n",
    "            \n",
    "        predictions.append(chain.invoke(input))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88613d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = predict_llm(train_with_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = predict_llm(test_with_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_candidates['Category_predicted'] = y_pred_train\n",
    "test_with_candidates['Category_predicted'] = y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78124fb",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc90c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On train data\n",
    "\n",
    "trainDf_Cleaned = pd.read_excel('data/train_cleaned.xlsx') \n",
    "train_accuarcy = Evaluate(trainDf_Cleaned,train_with_candidates[[\"Product_clean\", \"Category_predicted\"]])\n",
    "print(\"Accuarcy on train data\",round(train_accuarcy,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ed675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On test data\n",
    "\n",
    "testDf_Cleaned = pd.read_excel('data/test_cleaned.xlsx') \n",
    "test_accuarcy = Evaluate(testDf_Cleaned,test_with_candidates[[\"Product_clean\", \"Category_predicted\"]])\n",
    "print(\"Accuarcy on test data\",round(test_accuarcy,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd02ed2",
   "metadata": {},
   "source": [
    "### Exporte test rsults "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb88395",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_candidates.to_excel('data/llm_approach_test_results.xlsx',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-technique-chemosens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
